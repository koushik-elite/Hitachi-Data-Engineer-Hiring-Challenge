{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "import copy \n",
    "from imblearn.over_sampling import SMOTE \n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from sklearn import metrics\n",
    "from pathlib import Path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('DataSet')\n",
    "\n",
    "df_train = pd.read_csv(path/'Train.csv', low_memory=False)\n",
    "df_test = pd.read_csv(path/'Test.csv', low_memory=False)\n",
    "test_id_code = df_test.id_code\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_Volume = {\"high\": 2, \"medium\": 1, \"low\": 0 }\n",
    "Predict_Volume = {2: \"high\", 1: \"medium\", 0: \"low\" }\n",
    "is_weekend = {\"False\": 0 , \"True\": 1}\n",
    "\n",
    "def ChangeTime(x):\n",
    "    if x != \"\":\n",
    "        dt = datetime.datetime.strptime(x, \"%I:%M:%S %p\")\n",
    "        seconds = dt.second + dt.minute * 60 + dt.hour * 60 * 60\n",
    "        #print(seconds)\n",
    "        return int(seconds)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def ConvertStations(x):\n",
    "    if x != \"\":\n",
    "        x = x.replace(\"station$\", \"\")\n",
    "        return int(x)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def dummy_station_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "def Prepare(dataset):\n",
    "    dataset[\"current_time\"] = dataset[\"current_time\"].apply(ChangeTime)\n",
    "    dataset[\"source_name\"] = dataset[\"source_name\"].apply(ConvertStations)\n",
    "    dataset[\"destination_name\"] = dataset[\"destination_name\"].apply(ConvertStations)\n",
    "    dataset[\"is_weekend\"] = dataset[\"is_weekend\"].apply(lambda x: is_weekend[str(x)])\n",
    "    dataset = dataset.sort_values(by=['current_date', 'current_time'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Prepare(df_train)\n",
    "df_test = Prepare(df_test)\n",
    "df_train[\"target\"] = df_train[\"target\"].apply(lambda x: Target_Volume[x])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_empty(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_empty(df_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A. Haversine Distance Between the Two Lat/Lons:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_all_empty(dataset):\n",
    "    dataset[\"longitude_destination\"] = dataset[\"longitude_destination\"].fillna(0)\n",
    "    dataset[\"latitude_destination\"] = dataset[\"latitude_destination\"].fillna(0)\n",
    "    dataset[\"longitude_source\"] = dataset[\"longitude_source\"].fillna(0)\n",
    "    dataset[\"latitude_source\"] = dataset[\"latitude_source\"].fillna(0)\n",
    "    dataset[\"mean_halt_times_destination\"] = dataset[\"mean_halt_times_destination\"].fillna(0)\n",
    "    dataset[\"mean_halt_times_source\"] = dataset[\"mean_halt_times_source\"].fillna(0)\n",
    "    \n",
    "    dataset[\"country_code_source\"] = dataset[\"country_code_source\"].fillna(\"None\")\n",
    "    dataset[\"country_code_destination\"] = dataset[\"country_code_destination\"].fillna(\"None\")\n",
    "    \n",
    "    dataset[\"station_diff\"] = np.abs(dataset[\"source_name\"] - dataset[\"destination_name\"])\n",
    "    dataset.loc[:, 'center_latitude'] = (dataset['latitude_source'].values + dataset['latitude_destination'].values) / 2\n",
    "    dataset.loc[:, 'center_longitude'] = (dataset['longitude_source'].values + dataset['longitude_destination'].values) / 2\n",
    "    \n",
    "    # B. Manhattan Distance Between the two Lat/Lons:\n",
    "    dataset.loc[:, 'dummy_station_distance'] = dummy_station_distance(dataset['latitude_source'].values, dataset['longitude_source'].values, \n",
    "                                                     dataset['latitude_destination'].values,  dataset['longitude_destination'].values)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "df_train = fill_all_empty(df_train)\n",
    "df_test = fill_all_empty(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train X\", df_train.shape)\n",
    "print(\"Test X\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_difference(dataset):\n",
    "    dataset[\"delay_time\"] = 0\n",
    "    _dates = dataset[\"current_date\"].unique()\n",
    "    final_data = pd.DataFrame()\n",
    "    #print(_dates)\n",
    "    for date in _dates:\n",
    "        dataset_block = dataset.loc[dataset[\"current_date\"] == date]\n",
    "        delay_time = dataset_block[\"current_time\"].diff().values\n",
    "        dataset.loc[dataset[\"current_date\"] == date, \"delay_time\"] = delay_time\n",
    "        \"\"\"\n",
    "        dataset_count_df = pd.DataFrame({'current_date' : dataset_block[\"current_date\"].values, \n",
    "                                       'delay_time': delay_time})\n",
    "        final_data = pd.concat([final_data, dataset_count_df], ignore_index=True)\n",
    "        print(\"final_data X\", final_data.shape)\n",
    "        del dataset_count_df\n",
    "        \"\"\"\n",
    "    dataset[\"delay_time\"] = dataset[\"delay_time\"].fillna(0)\n",
    "    del final_data\n",
    "    return dataset\n",
    "\n",
    "df_train = create_time_difference(df_train)\n",
    "df_test = create_time_difference(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train X\", df_train.shape)\n",
    "print(\"Test X\", df_test.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset):\n",
    "    dataset[\"current_time\"] = StandardScaler().fit_transform(np.array(dataset['current_time']).reshape(-1, 1))\n",
    "    dataset[\"source_name\"] = StandardScaler().fit_transform(np.array(dataset['source_name']).reshape(-1, 1))\n",
    "    dataset[\"destination_name\"] = StandardScaler().fit_transform(np.array(dataset['destination_name']).reshape(-1, 1))\n",
    "    dataset[\"mean_halt_times_destination\"] = StandardScaler().fit_transform(np.array(dataset['mean_halt_times_destination']).reshape(-1, 1))\n",
    "    dataset[\"mean_halt_times_source\"] = StandardScaler().fit_transform(np.array(dataset['mean_halt_times_source']).reshape(-1, 1))\n",
    "    dataset[\"station_diff\"] = StandardScaler().fit_transform(np.array(dataset['station_diff']).reshape(-1, 1))\n",
    "    dataset[\"dummy_station_distance\"] = StandardScaler().fit_transform(np.array(dataset['dummy_station_distance']).reshape(-1, 1))\n",
    "    dataset[\"delay_time\"] = StandardScaler().fit_transform(np.array(dataset['delay_time']).reshape(-1, 1))\n",
    "    dataset[\"center_latitude\"] = StandardScaler().fit_transform(np.array(dataset['delay_time']).reshape(-1, 1))\n",
    "    \n",
    "    dataset[\"halt_times_diff\"] = np.abs(dataset[\"mean_halt_times_source\"] - dataset[\"mean_halt_times_destination\"])\n",
    "    return dataset\n",
    "\n",
    "df_train = normalize_data(df_train)\n",
    "df_test = normalize_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.groupby([\"current_date\"])[\"train_name\"].nunique()\n",
    "def other_feature_engineering(dataset):\n",
    "    train_count_df = pd.DataFrame({'total_train_count' : dataset.groupby( [ \"current_date\"] )[\"train_name\"].count()}).reset_index()\n",
    "    train_count_df[\"total_train_count\"] = StandardScaler().fit_transform(np.array(train_count_df['total_train_count']).reshape(-1, 1))\n",
    "    dataset = pd.merge(dataset, train_count_df, on='current_date', how='left')\n",
    "    del train_count_df\n",
    "    return dataset\n",
    "\n",
    "df_train = other_feature_engineering(df_train)\n",
    "df_test = other_feature_engineering(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_csv(\"tempsave.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[\"current_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train['current_time']);\n",
    "print(\"Skewness: %f\" % df_train['current_time'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['current_time'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.countplot(x=\"target\", data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_features(dataset):\n",
    "    dataset = dataset.drop([\"id_code\", \"current_date\"], axis=1)\n",
    "    dataset = dataset.drop([\"current_year\", \"current_week\"], axis=1)\n",
    "    dataset = dataset.drop([\"longitude_source\", \"latitude_source\", \"longitude_destination\", \"latitude_destination\"], axis=1)\n",
    "    return dataset\n",
    "\n",
    "df_train = remove_unwanted_features(df_train)\n",
    "df_test = remove_unwanted_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Heat map</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hello\n",
    "print(\"Train X\", df_train.shape)\n",
    "print(\"Test X\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedata = pd.concat([df_train, df_test], sort=False)\n",
    "mergedata_pandas = pd.get_dummies(mergedata)\n",
    "\n",
    "df_train = mergedata_pandas[:df_train.shape[0]]\n",
    "df_test = mergedata_pandas[df_train.shape[0]:]\n",
    "\n",
    "y = df_train.target # Target variable\n",
    "X = df_train.drop([\"target\"], axis=1)\n",
    "df_test = df_test.drop([\"target\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df_train)\n",
    "print(\"Train X\", X.shape)\n",
    "print(\"Test X\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25, random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Over Sampling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 42) \n",
    "X_train, y_train = sm.fit_resample(X_train, y_train) \n",
    "\n",
    "#X_train, y_train = ADASYN().fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df_train)\n",
    "print(\"Train SMOTE X\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "model = LogisticRegression(max_iter=400, solver='liblinear', multi_class='auto', class_weight='balanced')\n",
    "\n",
    "# fit the model with data\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.abs(model.predict(X_test))\n",
    "#print(Y_pred)\n",
    "#Y_pred = np.asarray([np.argmax(row) for row in Y_pred])\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LogisticRegression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test.values, Y_pred))\n",
    "results = [ y_test.values[i] == Y_pred[i] for i in range(len(Y_pred))]\n",
    "print(\"Positive \",results.count(True) / len(results))\n",
    "print(\"Negative \",results.count(False) / len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test.values, Y_pred))\n",
    "results = [ y_test.values[i] == Y_pred[i] for i in range(len(Y_pred))]\n",
    "print(\"Positive \",results.count(True) / len(results))\n",
    "print(\"Negative \",results.count(False) / len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test.values, Y_pred))\n",
    "results = [ y_test.values[i] == Y_pred[i] for i in range(len(Y_pred))]\n",
    "print(\"Positive \",results.count(True) / len(results))\n",
    "print(\"Negative \",results.count(False) / len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = model.predict(df_test)\n",
    "submission = pd.DataFrame({'id_code' : test_id_code.values, 'target' : y_pred_final})\n",
    "submission['target'] = submission['target'].astype('int64', copy=False)\n",
    "submission['target'] = submission['target'].apply(lambda x: Predict_Volume[x])    \n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
